{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZE9ypO0-X_k"
   },
   "outputs": [],
   "source": [
    "# !pip install -U accelerate\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install -U trl\n",
    "# !pip install -U peft\n",
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7mw7fvE-rl-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RBS9E-LS-bOp"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"vibhorag101/phr-mental-therapy-dataset-conversational-format-1024-tokens\", trust_remote_code=True)\n",
    "# dataset = dataset.shuffle(seed=42).select(range(10000))\n",
    "dataset02 = dataset['val'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTsupE_gBvlS",
    "outputId": "1c5b59bc-e5bd-4cd1-a6b6-988d38a7b91d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'identity_37265',\n",
       " 'messages': [{'content': \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\",\n",
       "   'role': 'system'},\n",
       "  {'content': \"I'm grateful for your help. I really need someone to talk to about him.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"I'm here for you. Can you tell me a bit more about what's been going on?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Well, things have been tense between us lately. We've been together for a while, but it feels like we're drifting apart. I don't know what to do.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"It sounds like you're concerned about the state of your relationship. Have you noticed any specific changes or behaviors that have contributed to this tension?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Yes, we used to spend more time together and really enjoy each other's company. But recently, he's been distant and preoccupied with work. We hardly have meaningful conversations anymore.\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'It must feel frustrating to feel disconnected from someone you care about. Have you tried expressing your concerns to him?',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"I've tried, but every time I bring it up, he either brushes it off or gets defensive. I don't want to make him feel attacked, but I also want him to understand how much this is affecting me.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"It's important for both partners to feel heard and validated in a relationship. Perhaps finding a calm moment to communicate your feelings without blaming him could help create a space for open dialogue.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"That's a good suggestion. I'll try to approach the conversation differently next time. But what if he still doesn't listen or understand?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"If he's unable to hear you, it might be worth considering seeking couples therapy. A neutral and supportive environment can help facilitate communication and understanding between both of you.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"I hadn't thought about couples therapy before, but I'm willing to try anything at this point. I want our relationship to work.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"It's admirable that you're willing to take steps to improve your relationship. Remember, it takes effort from both parties involved. Are there any other aspects of your relationship that you would like to address?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"I think trust has also become an issue. I've noticed him being secretive with his phone and spending more time on social media. It makes me question his fidelity.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Trust is an essential foundation in any relationship. It's natural to feel concerned when there are changes in behavior. Would you feel comfortable having an open and honest conversation with him about your feelings of mistrust?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"I'll give it a try, but it won't be easy. I'm afraid of what I might find out. What if he confirms my suspicions?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"It's understandable to feel anxious about the outcome. Remember, whatever happens, you have the strength to face it. Sometimes knowing the truth can pave the way for healing and growth in a relationship.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Thank you. Your words give me some hope. I'll gather the courage to have that conversation with him. I don't want to lose him, but I also need to prioritize my own well-being.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. Taking care of yourself is crucial in any relationship. Remember, I'm here to support you through this process. Is there anything else you'd like to discuss or any other concerns you have?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'No, I think that covers it for now. I appreciate your guidance and for being here to listen. It means a lot to me.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"I'm glad I could be here for you. Remember, change takes time, and progress can come in small steps. Stay open to growth and know that you deserve a fulfilling and healthy relationship.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]\n",
    "# print(format_prompt(dataset[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YXPkEEqs-vuC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5aoPPaM__-J",
    "outputId": "b52f5709-39d4-4e0c-ac87-252526894087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e2951e8941f14af39db0b9a963f8b934",
      "d26f1901e00f4dc0b02d7c9a511a0e27",
      "95963deea5284640bb044dee23496178",
      "4afa46d34e144d08809b9ebbd6c32bb6",
      "efbfbc07f57d4e558f22a2322a789f73",
      "8dd1684cfc0546cd8a728af59a10ac86",
      "709e774c107e4ac7b20e4db5dc9b20be",
      "35b58f570f8244619d001efd9c4ca836",
      "74837f69078e4cb9b3414f4a85d6c50c",
      "f6e0fb20800a49fdbc48ccb180b6e319",
      "70698ed8a25844f68d951ea75a1471b7"
     ]
    },
    "id": "C1Uj_ycrAraY",
    "outputId": "42814569-1b5a-428b-8d8e-c592138da5f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.</s>\n",
      "<|user|>\n",
      "I'm grateful for your help. I really need someone to talk to about him.</s>\n",
      "<|assistant|>\n",
      "I'm here for you. Can you tell me a bit more about what's been going on?</s>\n",
      "<|user|>\n",
      "Well, things have been tense between us lately. We've been together for a while, but it feels like we're drifting apart. I don't know what to do.</s>\n",
      "<|assistant|>\n",
      "It sounds like you're concerned about the state of your relationship. Have you noticed any specific changes or behaviors that have contributed to this tension?</s>\n",
      "<|user|>\n",
      "Yes, we used to spend more time together and really enjoy each other's company. But recently, he's been distant and preoccupied with work. We hardly have meaningful conversations anymore.</s>\n",
      "<|assistant|>\n",
      "It must feel frustrating to feel disconnected from someone you care about. Have you tried expressing your concerns to him?</s>\n",
      "<|user|>\n",
      "I've tried, but every time I bring it up, he either brushes it off or gets defensive. I don't want to make him feel attacked, but I also want him to understand how much this is affecting me.</s>\n",
      "<|assistant|>\n",
      "It's important for both partners to feel heard and validated in a relationship. Perhaps finding a calm moment to communicate your feelings without blaming him could help create a space for open dialogue.</s>\n",
      "<|user|>\n",
      "That's a good suggestion. I'll try to approach the conversation differently next time. But what if he still doesn't listen or understand?</s>\n",
      "<|assistant|>\n",
      "If he's unable to hear you, it might be worth considering seeking couples therapy. A neutral and supportive environment can help facilitate communication and understanding between both of you.</s>\n",
      "<|user|>\n",
      "I hadn't thought about couples therapy before, but I'm willing to try anything at this point. I want our relationship to work.</s>\n",
      "<|assistant|>\n",
      "It's admirable that you're willing to take steps to improve your relationship. Remember, it takes effort from both parties involved. Are there any other aspects of your relationship that you would like to address?</s>\n",
      "<|user|>\n",
      "I think trust has also become an issue. I've noticed him being secretive with his phone and spending more time on social media. It makes me question his fidelity.</s>\n",
      "<|assistant|>\n",
      "Trust is an essential foundation in any relationship. It's natural to feel concerned when there are changes in behavior. Would you feel comfortable having an open and honest conversation with him about your feelings of mistrust?</s>\n",
      "<|user|>\n",
      "I'll give it a try, but it won't be easy. I'm afraid of what I might find out. What if he confirms my suspicions?</s>\n",
      "<|assistant|>\n",
      "It's understandable to feel anxious about the outcome. Remember, whatever happens, you have the strength to face it. Sometimes knowing the truth can pave the way for healing and growth in a relationship.</s>\n",
      "<|user|>\n",
      "Thank you. Your words give me some hope. I'll gather the courage to have that conversation with him. I don't want to lose him, but I also need to prioritize my own well-being.</s>\n",
      "<|assistant|>\n",
      "You're welcome. Taking care of yourself is crucial in any relationship. Remember, I'm here to support you through this process. Is there anything else you'd like to discuss or any other concerns you have?</s>\n",
      "<|user|>\n",
      "No, I think that covers it for now. I appreciate your guidance and for being here to listen. It means a lot to me.</s>\n",
      "<|assistant|>\n",
      "I'm glad I could be here for you. Remember, change takes time, and progress can come in small steps. Stay open to growth and know that you deserve a fulfilling and healthy relationship.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    chat = example['messages']\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "print(format_prompt(dataset['train'][0])['text'])\n",
    "\n",
    "train_dataset = dataset['train'].map(format_prompt)\n",
    "eval_dataset = dataset02.map(format_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCqB0YAjCtxe",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model Configuration for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Cv3xD7DCT6j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3FjSb1StBoZB"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1zomMUVyDVlX"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "b8342af26a054f4d8b5ca77351ad60a9",
      "7f51a89292e0476dba465a7f0341dc59",
      "d9143a59b06e46fcb97156742a62a59d",
      "70c36bc487064fafbec74dc41e5d1c12",
      "e942ff5952b2417ca076dffb3582e147",
      "fbdaf136480b4222975ed1e1770d684d",
      "065628c00c964c449b15d586f7344a56",
      "5470a41bfa8141e8994452008d9c156a",
      "55e5e4fa84c64becbf190340a1011e53",
      "eec90680593b48f9bb84dbdf4deb0fee",
      "c2a3b7d380e749c7b75eb77ce3f6234d",
      "cc0fd9515afa44448853dd705a43a371",
      "590faff6d5f441198ab11eb92748fe10",
      "f522784636e24b468f61bdeff936af63",
      "d506ab3ad8c94655a03864e8aa64714d",
      "67e101c04e964623b987d62fb60ccd66",
      "15de4d17ea6d4ec3bde245daa69cdcdd",
      "48ea2077f244470e8b8337f0d5862fe2",
      "479eb5ca89eb4969b8e991d35b3e8899",
      "067adb6a8507455c866e23ce32aebd5c",
      "a1da7d46b43741aa888ac2dab92865a8",
      "5e858035b00a4d98891a5986b42d5786",
      "25af326c23a4490baff91e876bafa343",
      "cc53a46708ca4cf095c2b5f18c191272",
      "2ae549325c5e45f6a07169303b29e24a",
      "96e1f6df412a44e08dafc4cf6789d003",
      "d8a75e94575b436e875e1c1011b39d58",
      "e07ba16b92234d46b94ac6006bbf5bec",
      "86fcf3149a1b469394d87d4eecec6e00",
      "1d153d6a18d74260bf7ff411844e3a22",
      "0104db5688a14659acf6f0d6caf35b1e",
      "461d5b88bb4e4b838b6345a19e9c1906",
      "60bcb2ece22d4138b20c5c78309d9397"
     ]
    },
    "id": "Nf6wcG38D57d",
    "outputId": "a763890b-4393-4b99-a61c-b4128b19a3a6"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "v1zmJFjqFcDo"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbllWS3EEGPN",
    "outputId": "a8f6c5f4-76cd-40a3-d692-51abd53d04f2"
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew5TF7wCJWLC",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LoRA Configuration for PEFT Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jtXtig4AFisx"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CffEtrmwKYPv"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "b2c33a398e2f45eca19a2b54a47b5cc6",
      "e96355600e7442a2ab76b97d5f6b8b30",
      "58775d6f0525494c80f20812b3f255cc",
      "cce093ed251c4c24a9d2fed63d62fdef",
      "1f69109b2eae469a8fbf1475cac2d969",
      "c7d5f9ad8e174d1b8c4bafb4f800210e",
      "4bd8cce0b0bc43c8892973b6547c2247",
      "19b586a3f07e419d8db66479445a20bb",
      "82d41ca6c3f54baaae06c6a68ad03227",
      "2882e390f87f44f196fd71e07557940e",
      "98f67c5646b143dd91624b7cfc75f312"
     ]
    },
    "id": "H7jp4DpnKESd",
    "outputId": "119a653f-d650-4417-9a86-33da6a09e581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/mahfuj/miniconda3/envs/llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5570' max='5570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5570/5570 6:47:08, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.729117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.712700</td>\n",
       "      <td>0.697103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.679810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.666315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.663900</td>\n",
       "      <td>0.657730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.649343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>0.643451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.639027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.635131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.629317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.620100</td>\n",
       "      <td>0.626210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>0.622297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.626200</td>\n",
       "      <td>0.618231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.615839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.612200</td>\n",
       "      <td>0.612980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.610529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.608487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.605855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>0.603733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.600269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.598701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.597618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.596206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.594752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.592933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.590300</td>\n",
       "      <td>0.589571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.589300</td>\n",
       "      <td>0.589221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.593900</td>\n",
       "      <td>0.587710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.587274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>0.585618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.583062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>0.581005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>0.580746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.579885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.579155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.577464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.576041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.581700</td>\n",
       "      <td>0.574999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.574575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.572263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.570653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.570205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.569482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.567377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.567039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.562100</td>\n",
       "      <td>0.566085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>0.565105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0.563762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.562643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.561201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.564200</td>\n",
       "      <td>0.561248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.560100</td>\n",
       "      <td>0.559669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.559197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.557803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.557479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.558819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.556805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.556409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.555995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.519400</td>\n",
       "      <td>0.555466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.555677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.554864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.553870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.511800</td>\n",
       "      <td>0.553437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.552630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.552405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.551407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.551175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.510900</td>\n",
       "      <td>0.550305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.550468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>0.549551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.548701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>0.548266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.547408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.546863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.546796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>0.546350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.545896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.545194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.544365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.544255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.544025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>0.543126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.542941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.542194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.542088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.541801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.541482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.541451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.540836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>0.540786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>0.540376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.540379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.539907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.539883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.539596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.539512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.539450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.539146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.539124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>0.539030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.539032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.538958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.538812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.538766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.538742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.538723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>0.538713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.538706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.497600</td>\n",
       "      <td>0.538692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.538692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5570, training_loss=0.5532980939430223, metrics={'train_runtime': 24431.2012, 'train_samples_per_second': 1.824, 'train_steps_per_second': 0.228, 'total_flos': 1.4850670948297114e+17, 'train_loss': 0.5532980939430223, 'epoch': 1.9998204829009962})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "output_dir = \"train_dir_mental_health_assistant\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # max_steps=1500,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],\n",
    "\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=50,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m09BpqErMbmj"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora-mental-health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained PEFT Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora-mental-health\",\n",
    "    device_map = 'auto',\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "I feel very lonely and inadequate\n",
      "<|assistant|>\n",
      "I'm sorry to hear that. Loneliness and inadequacy can be tough emotions to deal with. Can you tell me more about what's been going on?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"\"\"<|user|>\n",
    "I feel very lonely and inadequate\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=merged_model, tokenizer=tokenizer)\n",
    "output = pipe(prompt)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
